---
layout: post
title: Implementations of Affinity in R, Hadoop Mapreduce and Spark
comments: true
---

Affinity analysis is a techinque that discovers co-occurence relationships among activities performed by specific individuals or groups. In retail it is used to perform basket analysis.
Basket analysis may tell a retailer that customers often purchases shampooo and conditioner together. [1]

This blog tries to implement this analysis using several popular techniques, R, Scala & Scalding in Apache Hadoop, and Scala & Apache Spark. The idea can also be implemented in other
techinques such as Java Mapreduce, Python, etc.

Let's look at the problem below.

## Problem
The sales data are recorded in a csv file. There are three fields, orderid, product and quantity. We will find out the quantity that every groups of product are purchased together (same order).
A small sample data are listed.

```
sales.csv

orderid, product, quantity -- Header not included

2,orange,5
2,grape,5
1,orange,6
1,peach,7
2,apple,3
2,peach,4
3,apple,3
3,peach,5
2,orange,6
1,grape,4
```

## Algorithms
We are going to use the Split-Apply-Combine method to solve the problem.
Split-Apply-Combine is a common data manipulation which has three phases:
* Splitting data by the value of one or more variables
* Applying a function to each chunk of data independently
* Combining the data back into one piece

For our task, 

* Split the sales data by orderid. Create a list of orderid-> (product, quantity)
* Calculate all the product combinations and their quantities in every order. Create (product group, quantity) list.
* Combine all the (product group, quantity) among all the order and sum up by grouping on the product groups. This gives the final
(product group, quantity) list.

## R Implementation

In R, there are many ways and packages that can do split-apply-combine. Here, we use split, lapply and aggregate functions. The comments in the code have clearly explained the algorithms.

```r
## Affinity.R
## Calculate affinity from a sales file for n-product group
##
affinityR <- function(fileName,n=2) {
  
  ## This function creates all the n-product combinations in an order.
  ## For every n-product group, sort the product in order, calculate  
  ## the quantity.
  ##
  combinations <- function(df,n) {
    ## Only calculate for orders having n and more products
    if( nrow(df) >=n ) {
      ## Create all the possible n-product group
      t1 <- combn(split(df[,2:3], df["product"]),n)
      
      t2 <- do.call(rbind.data.frame,lapply(1:ncol(t1), function(x) {
        p <- sapply(1:n, function(y) t1[[y,x]][["product"]] )
        c( sort(p), list( prod( sapply(1:n, function(z) t1[[z,x]]
		[["quantity"]]) )))
      } ) )
      names(t2) <- c(sapply(1:n, function(x) paste("product", x, 
		sep="")), "quantity")
      t2
    }
    else
      list()
  }
  
  ## Read data
  data <- read.csv(fileName, colClasses=c("integer", "character", 
	"integer"), header=FALSE)
  names(data) <- c("orderid","product","quantity")
  
  ## Group by orderid and quantity to ensure one product only occurs 
  ## once in a single orderid.
  ## This step can be bypassed if the original data has satisfied  
  ## this requirement.
  d1 <- aggregate(data$quantity, by=list(data$orderid, data$product), 
	FUN=sum)
  names(d1) <- c("orderid","product","quantity")
  
  ## Split by orderid and then apply the combinations function on 
  ## each group
  d2<- do.call(rbind.data.frame, lapply(split(d1, d1["orderid"]), 
	combinations, n=n))
  
  ## Sum up all groups 
  d3 <- aggregate( d2$quantity, by=d2[,sapply(1:n, function(x) 
    paste("product", x, sep=""))], FUN=sum)
  
  names(d3)[ length(names(d3)) ] <- "quantity"
  
  ## Output
  d3[ with(d3, order(-quantity)), ]
}
```

In the codes, R function split() is used to split the data. Then function sapply() is used to apply the combinations() UDF to create all product groups in 
an order. At the end, R function aggregate() is called combine to the results.

Some results:

```
> source('C:/Users/yhuang/git/blog/affinity/workspace/AffinityR/affinity.R')
> affinityR("C:/Users/yhuang/git/blog/affinity/workspace/data/sales.csv", 2)
  product1 product2 quantity
4   orange    peach       86
1    grape   orange       79
3    grape    peach       48
2    apple   orange       33
5    apple    peach       27
6    apple    grape       15
> affinityR("C:/Users/yhuang/git/blog/affinity/workspace/data/sales.csv", 3)
  product1 product2 product3 quantity
1    grape   orange    peach      388
4    apple    grape   orange      165
2    apple   orange    peach      132
3    apple    grape    peach       60
>
```

## Hadoop Mapreduce Implementation

When the data size is huge, we may think of a Big Data solution, such as Apache Hadoop. Hadoop uses the Mapreduce framework for distributed computing.

A Hadoop Mapreduce contains a map phase and a reduce phase. 

There are many ways to create a Mapreduce program. Here Scala and Scalding are used as I think this option can show the beauty of functional programming and make the solution more compact than the native Java Mapreduce.

The source code is below.

```scala
//
// AffinityHadoop.scala
//
import com.twitter.scalding.{Job, Args, TextLine, Csv}
import cascading.pipe.Pipe

class AffinityHadoop(args : Args) extends Job(args) {
  val logSchema = List('orderid, 'product, 'quanity)
  
  val input = Csv("/home/yhuang/blog/affinity/workspace/data/sales.csv",
    ",", logSchema).read
  val n = args("n").toInt
  val output = affinityHadoop(input, n)
  output.write( Csv("/home/yhuang/blog/affinity/workspace/data/affinity_hadoop.csv"))
  
  def affinityHadoop(pipe:Pipe, n:Int) = {
    pipe
      // Sum quantity by orderid and product
      .groupBy(('orderid, 'product))( group =>
        group.sum[Int]('quanity -> 'quantity)
      )
      // Group product and quantity by order
      .groupBy('orderid) { group =>
        group.toList[(String, Int)]( ('product, 'quantity)-> 'prods )
        
      }
      // Calculate all combinations in every groups
      // Calculate quantity for every combinations
      .flatMapTo('prods -> ('pg, 'quantity) ){
	prods: List[(String,Int)] =>
        prods.combinations(n)
          .map{ c => 
            c.sortBy(_._1)
              .foldLeft(("",1))
		( (prev: (String,Int), cur :(String, Int)) => 
              (prev._1 + "|" + cur._1, prev._2*cur._2 ) )
          }
      }
      // Sum quantity by product group
      .groupBy('pg)(_.sum[Int]('quantity -> 'quantity))
      .groupAll { _.sortBy('quantity).reverse}
     
  }
}
```

In the Scala codes above, groupBy() and map() are used to implement the Split-Apply-Combine Algorithms. There is no clear boundary between them.
When the data file is read, a scalding Pipe object is created, which contains the data in records.

When a groupBy() is used, the data is split into groups. Then the group functions can be used to implement the Apply stage or the Combine stage.

The results:

```
// n=2
|orange|peach,86
|grape|orange,79
|grape|peach,48
|apple|orange,33
|apple|peach,27
|apple|grape,15

// n=3
|grape|orange|peach,388
|apple|grape|orange,165
|apple|orange|peach,132
|apple|grape|peach,60
```

## Spark Implementation

Another distributed computing option is Apache Spark. Spark can cache the intermediate results in memory and so that can achieve very fast speed. Scala is used in this example.

The code is below.

```scala
//
// AffinitySpark.scala
//
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.rdd._

object AffinitySpark {
    def main(args: Array[String]) {
      
      val inputFile = args(0)
      val n = args(1).trim().toInt
      
      val conf = new SparkConf().setAppName("Affinity")
	.setMaster("local")
      val sc = new SparkContext(conf)
            
      val input = sc.textFile(inputFile)

      val output = affinitySpark(input, n)
      
      output.foreach(println)
    }
    
    def affinitySpark(d1 : RDD[String], n:Int) = {
      // Split csv into fields, key is order id no & product id
      val d2 = d1.map( line => {
        val t = line.split(",")
        ((t(0).trim.toInt, t(1).trim), t(2).trim.toInt)
      })
      
      // Combine same product in same order
      val d3 = d2.reduceByKey( (a,b) => a+b)
      
      // Convert to key to order id
      val d4 = d3.map( x => (x._1._1, (x._1._2, x._2)))
      
      // Create all combinations in every orders
      val d5 = d4.groupByKey()
      val d6 = d5.map( x => x._2.toList.combinations(n) )
      
      // In every order, create the product set as key and quantity
      val d7 = d6.flatMap(y => y.map( c => {
        c.sortBy(_._1)
              .foldLeft(("",1))
		( (prev: (String,Int), cur :(String, Int)) => 
              (prev._1 + "|" + cur._1, prev._2*cur._2 ) )
      } ))

      // Sum up by every product sets
      val d8 = d7.reduceByKey((a,b) => a+b).collect().sortBy(_._2)
	.reverse
      d8
    }
}
```

The Spark APIs look similar to Scalding APIs. The comments and codes are pretty self-contained.

Some results:

```
// n=2
(|orange|peach,86)
(|grape|orange,79)
(|grape|peach,48)
(|apple|orange,33)
(|apple|peach,27)
(|apple|grape,15)

// n=3
(|grape|orange|peach,388)
(|apple|grape|orange,165)
(|apple|orange|peach,132)
(|apple|grape|peach,60)
```

## Discussion

I had tried to use MDX and DAX of SQL Server Analysis Service to implement the basket affinity in a project. It was not pleasant experiences anyway:). 

The Split-Apply-Combine methodology makes the problem quite straightforward.

Quite often, we use R for ad-hoc analysis, Hadoop for huge amount of data and batch processing. Spark, claimed by its speed, is a good option to real-time/ fast speed environment.
But this is out of scope of this writing. Three options are provided to try to adopt the same method in different environments.

Please note that the implementations in this writing may not be fully efficient, for example, in the Spark codes, no cache is used. Also I have not got a chance to test and compare with a big dataset in cluster. If anyone can have a try, please kindly provide your feedback.

All codes in this writing can be found in my [github](https://github.com/yorkhuang-au/blog).

## References:

[1] [Wikipedia page](http://en.wikipedia.org/wiki/Affinity_analysis)

{% include twitter_plug.html %}

